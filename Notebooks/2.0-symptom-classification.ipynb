{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symptom classification EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Dataset path \"\"\"\n",
    "dataset_path = \"/Users/aic/Documents/lara2018-master/classifier/dataset/symptom/\"\n",
    "os.path.exists(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.RandomAffine(degrees=10,translate=(0.1,0.1),scale=(0.9,1.1)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
    "        ])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ImageFolder(os.path.join(dataset_path,\"train\"),transform=train_transform)\n",
    "validation_data = ImageFolder(os.path.join(dataset_path,\"val\"),transform=eval_transform)\n",
    "test_data = ImageFolder(os.path.join(dataset_path,\"test\"),transform=eval_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(training_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "val_loader = DataLoader(validation_data,batch_size=BATCH_SIZE,shuffle=False)\n",
    "test_loader = DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet module prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effnet_b3():\n",
    "    # load efficientnet b3 ImageNet weights\n",
    "    model = models.efficientnet_b3(pretrained=True)\n",
    "    for i,layer in enumerate(model.children()):\n",
    "        if i == 0:\n",
    "            for j, param in enumerate(layer.parameters()):\n",
    "                if j < 300:\n",
    "                    param.requires_grad = False\n",
    "                \n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3,inplace=True),\n",
    "        nn.Linear(in_features,5,bias=True),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score,precision_score,f1_score,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(training_data.class_to_idx.values())\n",
    "y = training_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "from sklearn.utils import class_weight\n",
    "class_weights  = class_weight.compute_class_weight(\"balanced\",classes=classes,y=y)\n",
    "class_weights = [1, 0.92028986, 0.54899135, 1.07932011, 1.43773585]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set devices\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdadmin/miniconda3/envs/torch_env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rdadmin/miniconda3/envs/torch_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/200: 100%|█████████████████████████████████████████████████████████████████████| 60/60 [00:14<00:00,  4.28it/s, acc=0.697, f1=0.727, loss=1.33]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.62it/s, va_acc=0.917, va_f1=0.879, val_loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss improved from inf to 1.046172375862415, saving model to: /home/rdadmin/Documents/Fraol-Projects/AIC_Coffee_Disease_DL/classifier_weights/effnet_classifier_20230306_071507.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200: 100%|█████████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.67it/s, acc=0.901, f1=0.895, loss=1.05]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.80it/s, va_acc=0.921, va_f1=0.885, val_loss=0.989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss improved from 1.046172375862415 to 0.9885035432302035, saving model to: /home/rdadmin/Documents/Fraol-Projects/AIC_Coffee_Disease_DL/classifier_weights/effnet_classifier_20230306_071507.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200: 100%|█████████████████████████████████████████████████████████████████████| 60/60 [00:13<00:00,  4.33it/s, acc=0.921, f1=0.916, loss=1.01]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.49it/s, va_acc=0.927, va_f1=0.893, val_loss=0.973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss improved from 0.9885035432302035 to 0.9729503163924584, saving model to: /home/rdadmin/Documents/Fraol-Projects/AIC_Coffee_Disease_DL/classifier_weights/effnet_classifier_20230306_071507.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200: 100%|████████████████████████████████████████████████████████████████████| 60/60 [00:14<00:00,  4.25it/s, acc=0.936, f1=0.934, loss=0.988]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.69it/s, va_acc=0.942, va_f1=0.914, val_loss=0.968]\n",
      "Epoch 5/200: 100%|████████████████████████████████████████████████████████████████████| 60/60 [00:11<00:00,  5.09it/s, acc=0.944, f1=0.942, loss=0.974]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.81it/s, va_acc=0.938, va_f1=0.908, val_loss=0.964]\n",
      "Epoch 6/200: 100%|████████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.93it/s, acc=0.947, f1=0.944, loss=0.968]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.78it/s, va_acc=0.944, va_f1=0.917, val_loss=0.959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss improved from 0.9729503163924584 to 0.9592429261941177, saving model to: /home/rdadmin/Documents/Fraol-Projects/AIC_Coffee_Disease_DL/classifier_weights/effnet_classifier_20230306_071507.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200: 100%|████████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.65it/s, acc=0.949, f1=0.946, loss=0.967]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.91it/s, va_acc=0.946, va_f1=0.92, val_loss=0.961]\n",
      "Epoch 8/200: 100%|█████████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.89it/s, acc=0.952, f1=0.95, loss=0.961]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.90it/s, va_acc=0.95, va_f1=0.927, val_loss=0.96]\n",
      "Epoch 9/200: 100%|████████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.96it/s, acc=0.964, f1=0.961, loss=0.952]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.74it/s, va_acc=0.942, va_f1=0.914, val_loss=0.965]\n",
      "Epoch 10/200: 100%|███████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.82it/s, acc=0.964, f1=0.963, loss=0.949]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.67it/s, va_acc=0.941, va_f1=0.913, val_loss=0.963]\n",
      "Epoch 11/200: 100%|████████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.84it/s, acc=0.963, f1=0.961, loss=0.95]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.52it/s, va_acc=0.939, va_f1=0.91, val_loss=0.965]\n",
      "Epoch 12/200: 100%|███████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.80it/s, acc=0.969, f1=0.968, loss=0.944]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.70it/s, va_acc=0.944, va_f1=0.918, val_loss=0.962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 3.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200: 100%|███████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.81it/s, acc=0.968, f1=0.966, loss=0.945]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.72it/s, va_acc=0.941, va_f1=0.913, val_loss=0.961]\n",
      "Epoch 14/200: 100%|███████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.79it/s, acc=0.966, f1=0.964, loss=0.943]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.65it/s, va_acc=0.944, va_f1=0.918, val_loss=0.957]\n",
      "Epoch 15/200: 100%|████████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.92it/s, acc=0.97, f1=0.969, loss=0.942]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.85it/s, va_acc=0.946, va_f1=0.921, val_loss=0.958]\n",
      "Epoch 16/200: 100%|███████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.95it/s, acc=0.974, f1=0.972, loss=0.939]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.82it/s, va_acc=0.946, va_f1=0.921, val_loss=0.959]\n",
      "Epoch 17/200: 100%|███████████████████████████████████████████████████████████████████| 60/60 [00:12<00:00,  4.95it/s, acc=0.967, f1=0.965, loss=0.944]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.83it/s, va_acc=0.951, va_f1=0.928, val_loss=0.957]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Stopped!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class TrainCoffeeSymptomDetection:\n",
    "    \"\"\" Hyperparameters \"\"\"\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 3e-4\n",
    "    NUM_EPOCHS = 200\n",
    "\n",
    "    \"\"\" Temp values\"\"\"\n",
    "    BEST_VAL_LOSS = float(\"inf\")\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self):\n",
    "        # Load data\n",
    "        model = effnet_b3()\n",
    "        # load model and choose device\n",
    "        model.to(device)\n",
    "\n",
    "        \"\"\" Training loop \"\"\"\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights,device=device,dtype=torch.float32))\n",
    "        optimizer = optim.Adam(model.parameters(),lr=self.LEARNING_RATE)\n",
    "        # Learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",factor=0.1,patience=5,verbose=True)\n",
    "\n",
    "        for epoch in range(self.NUM_EPOCHS):\n",
    "            train_loader = DataLoader(training_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "            \n",
    "            epoch_train_loss = []\n",
    "            epoch_val_loss = []\n",
    "        \n",
    "            epoch_train_F1 = []\n",
    "            epoch_val_F1 = []\n",
    "\n",
    "            epoch_train_acc = []\n",
    "            epoch_val_acc = []\n",
    "            \n",
    "            model.train()\n",
    "            # Create a progress bar\n",
    "            loop = tqdm(enumerate(train_loader),total=len(train_loader))\n",
    "            \n",
    "            # Train on the training set\n",
    "            for batch_idx,data in loop:\n",
    "                image,target = data\n",
    "    \n",
    "                image = image.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                preds = model(image)\n",
    "             \n",
    "                loss = criterion(preds,target)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step() \n",
    "                #preds = torch.softmax(preds,dim=1)\n",
    "                preds = torch.argmax(preds,dim=1)\n",
    "                \n",
    "                acc = accuracy_score(preds.detach().cpu().numpy(),target.detach().cpu().numpy())\n",
    "                f1 = f1_score(preds.detach().cpu().numpy(),target.detach().cpu().numpy(),average=\"weighted\")\n",
    "                \n",
    "                epoch_train_loss.append(loss.detach().item())\n",
    "                epoch_train_acc.append(acc)\n",
    "                epoch_train_F1.append(f1)\n",
    "\n",
    "                loop.set_description(f\"Epoch {epoch+1}/{self.NUM_EPOCHS}\")\n",
    "                \n",
    "                loop.set_postfix(loss=np.array(epoch_train_loss).mean(),\n",
    "                                    acc = np.array(epoch_train_acc).mean(),\n",
    "                                    f1 = np.array(epoch_train_F1).mean())\n",
    "                loop.update()\n",
    "            # Test on the validation set\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                eval_loop = tqdm(enumerate(val_loader),total=len(val_loader))\n",
    "                for i,data in eval_loop:\n",
    "\n",
    "                    image,target = data\n",
    "                    image = image.to(device)\n",
    "                    target = target.to(device)\n",
    "                \n",
    "                    preds = model(image)\n",
    "\n",
    "                    loss = criterion(preds,target)\n",
    "                    epoch_val_loss.append(loss.detach().item())\n",
    "                    \n",
    "                    #preds = torch.softmax(preds,dim=1)\n",
    "                    preds = torch.argmax(preds,dim=1)\n",
    "                    \n",
    "                    acc = accuracy_score(preds.detach().cpu().numpy(),target.detach().cpu().numpy())\n",
    "                    f1 = f1_score(preds.detach().cpu().numpy(),target.detach().cpu().numpy(),average=\"weighted\")\n",
    "                    #recall = recall_score(preds.detach().cpu().numpy(),labels.detach().cpu().numpy(),average=\"weighted\",zero_division=0)\n",
    "                \n",
    "                    epoch_val_loss.append(loss.detach().item())\n",
    "                    epoch_val_acc.append(acc)\n",
    "                    epoch_val_F1.append(f1)\n",
    "\n",
    "                    eval_loop.set_description(f\"Validation\")\n",
    "\n",
    "                    eval_loop.set_postfix(val_loss=np.array(epoch_val_loss).mean(),\n",
    "                                            va_acc = np.array(epoch_val_acc).mean(),\n",
    "                                            va_f1 = np.array(epoch_val_F1).mean())\n",
    "                    eval_loop.update()\n",
    "        \n",
    "            avg_vloss = np.mean(epoch_val_loss)\n",
    "            scheduler.step(torch.mean(torch.tensor(epoch_val_loss),dtype=torch.float))\n",
    "            \n",
    "            \n",
    "            if self.early_stopping(avg_vloss,patience=10,model=model,epoch=epoch):\n",
    "                break\n",
    "\n",
    "\n",
    "    def early_stopping(self,val_loss,patience=5,min_delta=0.01,model=None,epoch=0):\n",
    "        \"\"\" Early stopping \"\"\"\n",
    "        if self.BEST_VAL_LOSS - val_loss > min_delta:\n",
    "            model_path = f\"/home/rdadmin/Documents/Fraol-Projects/AIC_Coffee_Disease_DL/classifier_weights/effnet_classifier_{self.timestamp}.pth\"\n",
    "            print(f\"val_loss improved from {self.BEST_VAL_LOSS} to {val_loss}, saving model to: {model_path}\")\n",
    "            self.BEST_VAL_LOSS = val_loss\n",
    "            torch.save(model.state_dict(),model_path)\n",
    "            self.best_epoch = epoch\n",
    "            return False\n",
    "\n",
    "        if epoch - self.best_epoch > patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            print(f\"Stopped!!!\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TrainCoffeeSymptomDetection().train_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model,dataloader):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    probablities = []\n",
    "    progress_bar = tqdm(enumerate(dataloader),total=len(dataloader),desc='Evaluating')\n",
    "    model.eval()\n",
    "    for i,data in progress_bar:\n",
    "        image,target = data\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "    \n",
    "        preds = model(image)\n",
    "\n",
    "        true_labels += target.tolist()\n",
    "        probablities += preds.tolist()\n",
    "  \n",
    "        preds = torch.argmax(preds,dim=1)\n",
    "        \n",
    "        pred_labels += preds.tolist()\n",
    "\n",
    "    return true_labels,pred_labels,probablities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = effnet_b3()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"../classifier_weights/effnet_classifier_20230306_071507.pth\",map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:59<00:00,  4.56s/it]\n"
     ]
    }
   ],
   "source": [
    "Y_true,y_pred,prob = evaluate(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        38\n",
      "           1       0.93      0.92      0.93        90\n",
      "           2       0.98      0.97      0.98       149\n",
      "           3       0.95      0.95      0.95        75\n",
      "           4       0.88      0.91      0.90        57\n",
      "\n",
      "    accuracy                           0.95       409\n",
      "   macro avg       0.95      0.95      0.95       409\n",
      "weighted avg       0.95      0.95      0.95       409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_true,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize and export as pytorch lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1,3,224,224,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch/jit/_trace.py:992: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 5 / 5 (100.0%)\n",
      "Greatest absolute difference: 0.09118932485580444 at index (0, 2) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 0.467029368680333 at index (0, 0) (up to 1e-05 allowed)\n",
      "  _check_trace(\n"
     ]
    }
   ],
   "source": [
    "coffeenet_torchscript = torch.jit.trace(model,dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then optimize the TorchScript formatted model for mobile and save it:\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "coffeenet_torchscript_optimized = optimize_for_mobile(coffeenet_torchscript)\n",
    "torch.jit.save(coffeenet_torchscript_optimized, \"coffeenet_quantized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
